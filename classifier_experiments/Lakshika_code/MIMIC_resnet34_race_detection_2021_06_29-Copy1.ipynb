{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting image-classifiers==1.0.0b1\n",
      "  Downloading image_classifiers-1.0.0b1.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: image-classifiers\n",
      "  Building wheel for image-classifiers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for image-classifiers: filename=image_classifiers-1.0.0b1-py3-none-any.whl size=19954 sha256=acb588c4e7f1fe8bd195b0b812c2197a5ee493eaee8dc07a938f036fd79cb888\n",
      "  Stored in directory: /root/.cache/pip/wheels/bd/0d/8f/a84087f2ce6e91102d92816969d7cfe6179e6e3832199bf63b\n",
      "Successfully built image-classifiers\n",
      "Installing collected packages: image-classifiers\n",
      "Successfully installed image-classifiers-1.0.0b1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: keras-applications in /usr/local/lib/python3.6/dist-packages (1.0.8)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras-applications) (1.19.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras-applications) (1.15.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow==2.5.0\n",
    "import math\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Input, Dense, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, accuracy_score, recall_score, precision_score, f1_score\n",
    "import random as python_random\n",
    "\n",
    "!pip install image-classifiers==1.0.0b1\n",
    "!pip install keras-applications\n",
    "from classification_models.tfkeras import Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retinal Scans - Fundus Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2021)\n",
    "python_random.seed(2021)\n",
    "tf.random.set_seed(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Number of devices: 1\n",
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/mixed_precision/loss_scale.py:52: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale\n"
     ]
    }
   ],
   "source": [
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(mirrored_strategy.num_replicas_in_sync))\n",
    "\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "ls_A = os.listdir(\"datasets/rop_seg_data/train/white\")\n",
    "ls_B = os.listdir(\"datasets/rop_seg_data/train/black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ls_A)\n",
    "ls_A.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ls_B)\n",
    "ls_B.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(len(ls_A)):\n",
    "    data.append([\"white/\"+ls_A[i], \"WHITE\"])\n",
    "    \n",
    "for i in range(len(ls_B)):\n",
    "    data.append([\"black/\"+ls_B[i], \"BLACK/AFRICAN AMERICAN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.DataFrame(data, columns=[\"image_id\", \"race\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.to_csv(\"rop_data_fundus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('rop_data_fundus.csv')\n",
    "data_df = data_df[data_df.race.isin(['WHITE','BLACK/AFRICAN AMERICAN'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = data_df[:int(len(data_df)*8/10)]\n",
    "valid_df = data_df[int(len(data_df)*8/10):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WHITE                     0.627139\n",
       "BLACK/AFRICAN AMERICAN    0.372861\n",
       "Name: race, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.race.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "818"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT, WIDTH = 256, 256\n",
    "\n",
    "arc_name = \"ROP-\" + str(HEIGHT) + \"x\" + str(WIDTH) + \"80-20-split-resnet18-Float16_2-race_detection_rop_seg_data_fundus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18, preprocess_input = Classifiers.get('resnet18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet18_imagenet_1000_no_top.h5\n",
      "44924928/44920640 [==============================] - 3s 0us/step\n",
      "44933120/44920640 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "with mirrored_strategy.scope():\n",
    "    input_a = Input(shape=(HEIGHT, WIDTH, 3))\n",
    "    base_model = resnet18(input_tensor=input_a, include_top=False, input_shape=(HEIGHT,WIDTH,3), weights='imagenet')\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dense(2, name='dense_logits')(x)\n",
    "    output = Activation('softmax', dtype='float32', name='predictions')(x)\n",
    "    model = Model(inputs=[input_a], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.2'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:tf.keras.mixed_precision.experimental.LossScaleOptimizer is deprecated. Please use tf.keras.mixed_precision.LossScaleOptimizer instead. For example:\n",
      "  opt = tf.keras.mixed_precision.LossScaleOptimizer(opt)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "momentum_val=0.9\n",
    "decay_val= 0.0\n",
    "batch_s = 128 # may need to reduce batch size if OOM error occurs\n",
    "train_batch_size = batch_s\n",
    "test_batch_size = 128\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.1, patience=2, min_lr=1e-5, verbose=1)\n",
    "\n",
    "adam_opt = tf.keras.optimizers.Adam(learning_rate=learning_rate, decay=decay_val)\n",
    "adam_opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(adam_opt, \"dynamic\")\n",
    "\n",
    "\n",
    "with mirrored_strategy.scope():\n",
    "    model.compile(optimizer=adam_opt,\n",
    "                    loss=tf.losses.CategoricalCrossentropy(),\n",
    "                    metrics=[\n",
    "                        tf.keras.metrics.AUC(curve='ROC', name='ROC-AUC'),\n",
    "                        tf.keras.metrics.AUC(curve='PR', name='PR-AUC')\n",
    "                    ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(\n",
    "            rotation_range=15,\n",
    "            fill_mode='constant',\n",
    "            horizontal_flip=True,\n",
    "            zoom_range=0.1,\n",
    "            preprocessing_function=preprocess_input\n",
    "            )\n",
    "\n",
    "validate_gen = ImageDataGenerator(preprocessing_function=preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3268 validated image filenames belonging to 2 classes.\n",
      "Found 818 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batches = train_gen.flow_from_dataframe(train_df, directory=\"datasets/rop_seg_data/train\", x_col=\"image_id\", y_col=\"race\", class_mode=\"categorical\",target_size=(HEIGHT, WIDTH),shuffle=True,seed=2021,batch_size=train_batch_size, dtype='float32')\n",
    "validate_batches = validate_gen.flow_from_dataframe(valid_df, directory=\"datasets/rop_seg_data/train\", x_col=\"image_id\", y_col=\"race\", class_mode=\"categorical\",target_size=(HEIGHT, WIDTH),shuffle=False,batch_size=test_batch_size, dtype='float32')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.image.DataFrameIterator at 0x7fbbb056ad68>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch = math.ceil(len(data_df) / train_batch_size)\n",
    "val_epoch = math.ceil(len(valid_df) / test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_date = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "ES = EarlyStopping(monitor='val_loss', mode='min', patience=4, restore_best_weights=True)\n",
    "checkloss = ModelCheckpoint(\"saved_models/\" + str(arc_name) + \"_rop_seg-\" + str(learning_rate) + \"_\" + var_date+\"_epoch:{epoch:03d}.hdf5\", monitor='val_loss', mode='min', verbose=1, save_best_only=True, save_weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "28/32 [=========================>....] - ETA: 58:29 - loss: 0.3453 - ROC-AUC: 0.9478 - PR-AUC: 0.9408  "
     ]
    }
   ],
   "source": [
    "model.fit(train_batches,\n",
    "          validation_data = validate_batches,\n",
    "            epochs=100,\n",
    "            steps_per_epoch=int(train_epoch),\n",
    "            workers=32,\n",
    "            max_queue_size=50,\n",
    "            shuffle=True,\n",
    "            callbacks=[checkloss, reduce_lr, ES]\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chest Scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counter = Counter(train_batches.classes)                          \n",
    "max_val = float(max(counter.values()))       \n",
    "train_class_weights = {class_id : max_val/num_images for class_id, num_images in counter.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_batches = validate_gen.flow_from_dataframe(validation_df, directory=\"../Public_Datasets/mimic-cxr-jpg/physionet.org/files/mimic-cxr-jpg/2.0.0/files/\", x_col=\"path\", y_col=\"race\", class_mode=\"categorical\",target_size=(HEIGHT, WIDTH),shuffle=False,batch_size=test_batch_size, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_date = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "ES = EarlyStopping(monitor='val_loss', mode='min', patience=4, restore_best_weights=True)\n",
    "checkloss = ModelCheckpoint(\"saved_models/\" + str(arc_name) + \"_CXR_LR-\" + str(learning_rate) + \"_\" + var_date+\"_epoch:{epoch:03d}_val_loss:{val_loss:.5f}.hdf5\", monitor='val_loss', mode='min', verbose=1, save_best_only=True, save_weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_batches,\n",
    "            validation_data=validate_batches,\n",
    "            epochs=100, class_weight = train_class_weights,\n",
    "            steps_per_epoch=int(train_epoch),\n",
    "            validation_steps=int(val_epoch),\n",
    "            workers=32,\n",
    "            max_queue_size=50,\n",
    "            shuffle=True,\n",
    "            callbacks=[checkloss, reduce_lr, ES]\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.race.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = \"saved_models/MIMIC-320x32060-10-30-split-resnet-Float16_3-race_detection_lowpass25_CXR_LR-0.001_20211103-193808_epoch:007_val_loss:0.24422.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_batches = validate_gen.flow_from_dataframe(test_df, directory = \"Lowpass25_Images/\",x_col=\"path\", y_col=\"race\", class_mode=\"categorical\",target_size=(HEIGHT, WIDTH),shuffle=False,batch_size=128, dtype='float32', interpolation='bilinear') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mirrored_strategy.scope():\n",
    "    multilabel_predict_test = model.predict(test_batches, max_queue_size=10, verbose=1, steps=math.ceil(len(test_df)/128), workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_predict_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prediction = multilabel_predict_test\n",
    "input_df = test_df\n",
    "input_prediction_df = pd.DataFrame(input_prediction)\n",
    "true_logits = pd.DataFrame()\n",
    "loss_log = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prediction_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_calc(input_prediction_df, input_df):\n",
    "    ground_truth = input_df.race\n",
    "    pathology_array=['ASIAN',\n",
    "        'BLACK/AFRICAN AMERICAN',\n",
    "        'WHITE'\n",
    "        ]\n",
    "    i=0\n",
    "    auc_array = []\n",
    "    for pathology in pathology_array:\n",
    "        new_truth = (ground_truth.str.contains(pathology)).apply(int)\n",
    "        print(new_truth.shape)\n",
    "        input_prediction_val = input_prediction_df[i]\n",
    "        val = input_prediction_val\n",
    "        AUC = roc_auc_score(new_truth, val)\n",
    "        #AUC = average_precision_score(new_truth, val, average=None)\n",
    "        true_logits.insert(i, i, new_truth, True)\n",
    "        auc_array.append(AUC)\n",
    "        i += 1\n",
    "        \n",
    "    progress_df = pd.DataFrame({'Study':pathology_array, 'AUC':auc_array})\n",
    "    print(progress_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_calc(input_prediction_df, input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_model = \"saved_models/MIMIC-256x25660-10-30-split-resnet-Float16_3-race_detection_CXR_LR-0.001_20211021-052440_epoch:016_val_loss:0.24378.hdf5\"\n",
    "test_model = \"saved_models/MIMIC-256x25660-10-30-split-resnet-Float16_3-race_detection_CXR_LR-0.001_20211021-052440_epoch:016_val_loss:0.24378.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "ls = os.listdir(\"pytorch-CycleGAN-and-pix2pix-master/results/white_black/test_23/images/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(len(ls)//6):\n",
    "    data.append([ls[6*i], \"WHITE\"])\n",
    "    data.append([ls[6*i+1], \"BLACK/AFRICAN AMERICAN\"])\n",
    "    data.append([ls[6*i+2], \"WHITE\"])\n",
    "    data.append([ls[6*i+3], \"BLACK/AFRICAN AMERICAN\"])\n",
    "    data.append([ls[6*i+4], \"WHITE\"])\n",
    "    data.append([ls[6*i+5], \"BLACK/AFRICAN AMERICAN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.DataFrame(data, columns=[\"path\", \"race\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.race.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches = validate_gen.flow_from_dataframe(combined, directory = \"pytorch-CycleGAN-and-pix2pix-master/results/white_black/test_25/images/\",x_col=\"path\", y_col=\"race\", class_mode=\"categorical\",target_size=(HEIGHT, WIDTH),shuffle=False,batch_size=128, dtype='float32', interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mirrored_strategy.scope():\n",
    "    multilabel_predict_test = model.predict(test_batches, max_queue_size=10, verbose=1, steps=math.ceil(len(combined)/128), workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_predict_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = multilabel_predict_test\n",
    "#result = model.predict(validate_batches, val_epoch)\n",
    "labels = np.argmax(result, axis=1)\n",
    "target_names = ['Asian','Black', 'White']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "\n",
    "for i in range(len(labels)//6):\n",
    "    l.append(labels[6*i:6*(i+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_correct = []\n",
    "predict_incorrect = []\n",
    "for i in range(len(l)):\n",
    "    if np.array_equal([1,2,2,1,2,1], l[i]):\n",
    "        predict_correct.append(i)\n",
    "    else:\n",
    "        predict_incorrect.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_probs = []\n",
    "white_probs = []\n",
    "for i in predict_correct:\n",
    "    white_probs.append(result[6*i][1])\n",
    "    black_probs.append(result[6*i+1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(black_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = {}\n",
    "for i in range(len(result)):\n",
    "    probs[i] = result[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls[6*25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[6*27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs= [15,18,20,23,25]\n",
    "white_probs = [3.8162336e-01, 1.2340636e-01, 6.4106297e-01, 6.5219790e-01, 7.1337724e-01]\n",
    "fakeA_white = [6.1822724e-01, 8.7649578e-01, 3.5890210e-01,  3.4773538e-01, 2.8654653e-01]\n",
    "fakeB_black = [2.4298365e-04, 2.9653276e-04,3.8822962e-04,2.2254302e-04, 1.8199782e-04]\n",
    "black_probs = [9.9974924e-01, 9.9969411e-01,9.9960214e-01,  9.9976617e-01, 9.9981230e-01]\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(epochs, white_probs)\n",
    "plt.plot(epochs, fakeA_white)\n",
    "plt.plot(epochs, black_probs)\n",
    "plt.plot(epochs, fakeB_black)\n",
    "\n",
    "plt.legend([\"fake_A predicted as Black\", \"fake_A predicted as White\",\"fake_B predicted as White\",\"fake_B predicted as Black\"])\n",
    "plt.title(\"Probabilities - Image (9c5e78e3-ac01ec60-ab719485-03063374-c488be42)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "path_fakeA = \"pytorch-CycleGAN-and-pix2pix-master/results/white_black/test_18/images/9c5e78e3-ac01ec60-ab719485-03063374-c488be42_fake_A.png\"\n",
    "path_fakeB = \"pytorch-CycleGAN-and-pix2pix-master/results/white_black/test_18/images/9c5e78e3-ac01ec60-ab719485-03063374-c488be42_fake_B.png\"\n",
    "path_realA = \"pytorch-CycleGAN-and-pix2pix-master/results/white_black/test_18/images/9c5e78e3-ac01ec60-ab719485-03063374-c488be42_real_A.png\"\n",
    "path_realB = \"pytorch-CycleGAN-and-pix2pix-master/results/white_black/test_18/images/9c5e78e3-ac01ec60-ab719485-03063374-c488be42_real_B.png\"\n",
    "\n",
    "path_fakeA2 = \"pytorch-CycleGAN-and-pix2pix-master/results/white_black/test_18/images/9c5e78e3-ac01ec60-ab719485-03063374-c488be42_fake_A.png\"\n",
    "path_fakeB2 = \"pytorch-CycleGAN-and-pix2pix-master/results/white_black/test_18/images/9c5e78e3-ac01ec60-ab719485-03063374-c488be42_fake_B.png\"\n",
    "path_realA2 = \"pytorch-CycleGAN-and-pix2pix-master/results/white_black/test_18/images/9c5e78e3-ac01ec60-ab719485-03063374-c488be42_real_A.png\"\n",
    "path_realB2 = \"pytorch-CycleGAN-and-pix2pix-master/results/white_black/test_18/images/9c5e78e3-ac01ec60-ab719485-03063374-c488be42_real_B.png\"\n",
    "\n",
    "img_fakeA = cv2.imread(path_fakeA)\n",
    "img_fakeB = cv2.imread(path_fakeB)\n",
    "img_realA = cv2.imread(path_realA)\n",
    "img_realB = cv2.imread(path_realB)\n",
    "\n",
    "img_fakeA2 = cv2.imread(path_fakeA2)\n",
    "img_fakeB2 = cv2.imread(path_fakeB2)\n",
    "img_realA2 = cv2.imread(path_realA2)\n",
    "img_realB2 = cv2.imread(path_realB2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_farb = (img_fakeA-img_realB)\n",
    "diff_fbra = (img_fakeB-img_realA)\n",
    "\n",
    "diff_farb2 = (img_fakeA2-img_realB2)\n",
    "diff_fbra2 = (img_fakeB2-img_realA2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image = img[...,0]\n",
    "from PIL import Image, ImageFilter\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.suptitle(\"Image-9c5e78e3-ac01ec60-ab719485-03063374-c488be42\")\n",
    "#\n",
    "plt.subplot(341),plt.imshow(img_fakeA, cmap = 'gray')\n",
    "plt.title('Fake_A image - Epoch18'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(342),plt.imshow(img_fakeB, cmap = 'gray')\n",
    "plt.title('Fake_B image - Epoch18'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(345),plt.imshow(img_realB, cmap = 'gray')\n",
    "plt.title('Real_B image - Epoch18'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(346),plt.imshow(img_realA, cmap = 'gray')\n",
    "plt.title('Real_A image - Epoch18'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(349),plt.imshow(diff_farb, cmap = 'gray')\n",
    "plt.title('fakeA-realB - Epoch18'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(3,4,10),plt.imshow(diff_fbra, cmap = 'gray')\n",
    "plt.title('fakeB-realA - Epoch18'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "plt.subplot(3,4,3),plt.imshow(img_fakeA2, cmap = 'gray')\n",
    "plt.title('Fake_A image - Epoch25'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(344),plt.imshow(img_fakeB2, cmap = 'gray')\n",
    "plt.title('Fake_B image - Epoch25'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(347),plt.imshow(img_realB2, cmap = 'gray')\n",
    "plt.title('Real_B image - Epoch25'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(348),plt.imshow(img_realA2, cmap = 'gray')\n",
    "plt.title('Real_A image - Epoch25'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(3,4,11),plt.imshow(diff_farb2, cmap = 'gray')\n",
    "plt.title('fakeA-realB - Epoch25'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(3,4,12),plt.imshow(diff_fbra2, cmap = 'gray')\n",
    "plt.title('fakeB-realA - Epoch25'), plt.xticks([]), plt.yticks([])\n",
    "#\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "path_fakeA = \"pytorch-CycleGAN-and-pix2pix-master/results/white_black/test_15_50img/images/001ccf5a-808a286a-b93f7df2-d12167ad-103d015e_fake_A.png\"\n",
    "path_fakeB = \"pytorch-CycleGAN-and-pix2pix-master/results/white_black/test_15_50img/images/001ccf5a-808a286a-b93f7df2-d12167ad-103d015e_fake_B.png\"\n",
    "path_realA = \"pytorch-CycleGAN-and-pix2pix-master/results/white_black/test_15_50img/images/001ccf5a-808a286a-b93f7df2-d12167ad-103d015e_real_A.png\"\n",
    "path_realB = \"pytorch-CycleGAN-and-pix2pix-master/results/white_black/test_15_50img/images/001ccf5a-808a286a-b93f7df2-d12167ad-103d015e_real_B.png\"\n",
    "\n",
    "img_fakeA = cv2.imread(path_fakeA, cv2.IMREAD_GRAYSCALE)\n",
    "img_fakeB = cv2.imread(path_fakeB, cv2.IMREAD_GRAYSCALE)\n",
    "img_realA = cv2.imread(path_realA, cv2.IMREAD_GRAYSCALE)\n",
    "img_realB = cv2.imread(path_realB,cv2.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = (x==y)\n",
    "f = z[z==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(f)/(256*256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1= np.int32(img_fakeA)\n",
    "\n",
    "image2= np.int32(img_realB)\n",
    "\n",
    "diff_farb = image1 - image2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1= np.int32(img_fakeB)\n",
    "\n",
    "image2= np.int32(img_realA)\n",
    "\n",
    "diff_fbra = image1 - image2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_farb = cv2.subtract(img_fakeA,img_realB) # #img_fakeA - img_realB #\n",
    "diff_fbra = img_fakeB - img_realA #cv2.subtract(img_fakeB,img_realA) # #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = diff_farb[diff_farb <0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1= np.int32(img_fakeA)\n",
    "\n",
    "image2= np.int32(img_realB)\n",
    "\n",
    "diff_farb = image1 - image2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image = img[...,0]\n",
    "from PIL import Image, ImageFilter\n",
    "plt.figure(figsize=(10,15))\n",
    "#plt.suptitle(\"25th Epoch (Image-001ccf5a-808a286a-b93f7df2-d12167ad-103d015e) With only one channel in case of difference images\")\n",
    "#\n",
    "plt.subplot(321),plt.imshow(img_fakeA, cmap = 'gray')\n",
    "plt.title('Fake_A image'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(322),plt.imshow(img_fakeB, cmap = 'gray'), plt.colorbar()\n",
    "plt.title('Fake_B image'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(323),plt.imshow(img_realB, cmap = 'gray')\n",
    "plt.title('Real_B image'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(324),plt.imshow(img_realA, cmap = 'gray')\n",
    "plt.title('Real_A image'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(325),plt.imshow(diff_farb, cmap = 'RdBu'), plt.colorbar()\n",
    "plt.title('fakeA-realB'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(326),plt.imshow(diff_fbra, cmap = 'RdBu'), plt.colorbar()\n",
    "plt.title('fakeB-realA'), plt.xticks([]), plt.yticks([])\n",
    "#\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "fig, axes = plt.subplots(2,2, figsize=(10,10))\n",
    "\n",
    "# the colormaps we'll be trying out\n",
    "cmap_list = ['RdPu', 'spring', 'PRGn', 'gnuplot']\n",
    "\n",
    "for ax, name in zip(axes.flatten(), cmap_list):\n",
    "    im = ax.imshow(img_fakeA, aspect='auto',  cmap=plt.get_cmap(name))\n",
    "    ax.yaxis.set_major_locator(plt.NullLocator())   # remove y axis ticks\n",
    "    ax.xaxis.set_major_locator(plt.NullLocator())   # remove x axis ticks\n",
    "    ax.set_aspect('equal', adjustable='box')        # make subplots square\n",
    "    ax.set_title(f'Cmap: {name}', fontsize=18)      # add a title to each\n",
    "    divider = make_axes_locatable(ax)               # make colorbar same size as each subplot\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "    plt.colorbar(im, cax=cax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"pytorch-CycleGAN-and-pix2pix-master/datasets/w2b/trainA/0a0a1d17-5fb4ffcc-a8ca1541-8c44d583-fd9c6388.jpg\"\n",
    "img = cv2.imread(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=img.ravel() >250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=x[x==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prediction = multilabel_predict_test\n",
    "input_df = combined\n",
    "input_prediction_df = pd.DataFrame(input_prediction)\n",
    "true_logits = pd.DataFrame()\n",
    "loss_log = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_calc(input_prediction_df, input_df):\n",
    "    ground_truth = input_df.race\n",
    "    pathology_array=['ASIAN',\n",
    "        'BLACK/AFRICAN AMERICAN',\n",
    "        'WHITE'\n",
    "        ]\n",
    "    i=0\n",
    "    auc_array = []\n",
    "    for pathology in pathology_array:\n",
    "    \n",
    "        new_truth = (ground_truth.str.contains(pathology)).apply(int)\n",
    "        input_prediction_val = input_prediction_df[i]\n",
    "        val = input_prediction_val\n",
    "        AUC = roc_auc_score(new_truth, val)\n",
    "        #AUC = average_precision_score(new_truth, val, average=None)\n",
    "        true_logits.insert(i, i, new_truth, True)\n",
    "        auc_array.append(AUC)\n",
    "        i += 1\n",
    "        \n",
    "    progress_df = pd.DataFrame({'Study':pathology_array, 'AUC':auc_array})\n",
    "    print(progress_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_calc(input_prediction_df, input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "#import h5py\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, precision_recall_curve\n",
    "from sklearn.metrics import auc, accuracy_score, recall_score, precision_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = multilabel_predict_test\n",
    "#result = model.predict(validate_batches, val_epoch)\n",
    "labels = np.argmax(result, axis=1)\n",
    "target_names = ['Asian','Black', 'White']\n",
    "\n",
    "print ('Classwise ROC AUC \\n')\n",
    "for p in list(set(labels)):\n",
    "    fpr, tpr, thresholds = roc_curve(test_batches.classes, result[:57408][:,p], pos_label = p)\n",
    "    auroc = round(auc(fpr, tpr), 2)\n",
    "    print ('Class - {} ROC-AUC- {}'.format(target_names[p], auroc))\n",
    "\n",
    "print (classification_report(test_batches.classes, labels[:57408], target_names=target_names))\n",
    "class_matrix = confusion_matrix(test_batches.classes, labels[:57408])\n",
    "\n",
    "#sns.heatmap(class_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "heat=sns.heatmap(class_matrix, annot=True, fmt='d', cmap='Blues',xticklabels=target_names,yticklabels=target_names)\n",
    "heat.set_xlabel(\"Predicted Label\")\n",
    "heat.set_ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
