{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50f4fe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import hickle\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image, ImageChops, ImageDraw\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.models\n",
    "import cv2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from tqdm import tqdm\n",
    "from numba import jit, cuda\n",
    "from utils import systemic_brightening\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb0c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set directory\n",
    "os.chdir(\"/users/riya/race/classifier_experiments\") # which one? yep\n",
    "\n",
    "# ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# import model\n",
    "segmentation_classifier = keras.models.load_model('models/MIMIC-256x25680-20-split-resnet-Float16_2-race_detection_rop_seg_data_rop_seg-0.001_20220321-054140_epoch:011.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09023ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_arr = np.zeros((4546, 256, 256, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb5fa7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_arr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7c16fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run through classifier\n",
    "\n",
    "@jit         \n",
    "def predict_on_images(img_path, preds_df, colname,\n",
    "                     skeleton, thresh_type, intensity_change, brighten_sum,\n",
    "                     csv_name = \"brightened_predictions\"): \n",
    "    \n",
    "    img_files = os.listdir(dataset_path)\n",
    "    num_images = len(img_files) - 4500\n",
    "    \n",
    "    id_arr = [0] * num_images\n",
    "    img_arr = np.zeros((num_images, 256, 256, 3))\n",
    "    \n",
    "    preds_arr = [0] * num_images\n",
    "    \n",
    "    for i in tqdm(range(num_images)):\n",
    "        arr = np.array(Image.open(img_path + img_files[i]))\n",
    "        resized = cv2.resize(arr, (256,256)) # must use 256\n",
    "        channels = np.repeat(resized[:, :, np.newaxis], 3, axis=2).reshape((256,256,3))\n",
    "\n",
    "        modified_img = systemic_brightening(channels, skeleton, thresh_type, intensity_change, brighten_sum,\n",
    "                                           image_size = (256, 256))\n",
    "        modified_img = np.array(modified_img) # .reshape((1,256,256,3)) np reshape, bc substitute? \n",
    "        # img_arr = np.append(img_arr, modified_img)\n",
    "        img_arr[i] = modified_img\n",
    "        \n",
    "        # print(img_arr.shape)\n",
    "                       \n",
    "        # getting id     \n",
    "        img_id = re.findall(r'\\d+', img_files[i])\n",
    "        id_arr[i] = img_id\n",
    "    \n",
    "    preds_df['id'] = id_arr\n",
    "  \n",
    "    # getting prediction    \n",
    "    prediction = segmentation_classifier(img_arr)\n",
    "    \n",
    "    for j in range(num_images):\n",
    "        preds_arr[j] = prediction.numpy()[j, 1] # returning the white prediction for each image\n",
    "  \n",
    "    preds_df[colname] = preds_arr           \n",
    "    preds_df.to_csv(preds_path + csv_name + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f469dbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = pd.DataFrame(columns = ['id', '30', '60', '90', '120', '150']) # from id I can get race\n",
    "\n",
    "dataset_path = \"/users/riya/race/dataset/segmentations/\"\n",
    "preds_path = \"/users/riya/race/classifier_experiments/predictions/experiment1_plus_systemic_brightening/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e25c267d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-add7a222728a>:3: NumbaWarning: \n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"predict_on_images\" failed type inference due to: Untyped global name 'tqdm': Cannot determine Numba type of <class 'type'>\n",
      "\n",
      "File \"<ipython-input-23-add7a222728a>\", line 16:\n",
      "def predict_on_images(img_path, preds_df, colname,\n",
      "    <source elided>\n",
      "    \n",
      "    for i in tqdm(range(num_images)):\n",
      "    ^\n",
      "\n",
      "  @jit\n",
      "<ipython-input-23-add7a222728a>:3: NumbaWarning: \n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"predict_on_images\" failed type inference due to: Cannot determine Numba type of <class 'numba.core.dispatcher.LiftedLoop'>\n",
      "\n",
      "File \"<ipython-input-23-add7a222728a>\", line 16:\n",
      "def predict_on_images(img_path, preds_df, colname,\n",
      "    <source elided>\n",
      "    \n",
      "    for i in tqdm(range(num_images)):\n",
      "    ^\n",
      "\n",
      "  @jit\n",
      "/usr/local/lib/python3.6/dist-packages/numba/core/object_mode_passes.py:152: NumbaWarning: Function \"predict_on_images\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\n",
      "File \"<ipython-input-23-add7a222728a>\", line 8:\n",
      "def predict_on_images(img_path, preds_df, colname,\n",
      "    <source elided>\n",
      "    \n",
      "    img_files = os.listdir(dataset_path)\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/usr/local/lib/python3.6/dist-packages/numba/core/object_mode_passes.py:162: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit https://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"<ipython-input-23-add7a222728a>\", line 8:\n",
      "def predict_on_images(img_path, preds_df, colname,\n",
      "    <source elided>\n",
      "    \n",
      "    img_files = os.listdir(dataset_path)\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "<ipython-input-23-add7a222728a>:3: NumbaWarning: \n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"predict_on_images\" failed type inference due to: Untyped global name 'tqdm': Cannot determine Numba type of <class 'type'>\n",
      "\n",
      "File \"<ipython-input-23-add7a222728a>\", line 16:\n",
      "def predict_on_images(img_path, preds_df, colname,\n",
      "    <source elided>\n",
      "    \n",
      "    for i in tqdm(range(num_images)):\n",
      "    ^\n",
      "\n",
      "  @jit\n",
      "/usr/local/lib/python3.6/dist-packages/numba/core/object_mode_passes.py:152: NumbaWarning: Function \"predict_on_images\" was compiled in object mode without forceobj=True.\n",
      "\n",
      "File \"<ipython-input-23-add7a222728a>\", line 16:\n",
      "def predict_on_images(img_path, preds_df, colname,\n",
      "    <source elided>\n",
      "    \n",
      "    for i in tqdm(range(num_images)):\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/usr/local/lib/python3.6/dist-packages/numba/core/object_mode_passes.py:162: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit https://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"<ipython-input-23-add7a222728a>\", line 16:\n",
      "def predict_on_images(img_path, preds_df, colname,\n",
      "    <source elided>\n",
      "    \n",
      "    for i in tqdm(range(num_images)):\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      " 48%|████▊     | 22/46 [00:04<00:04,  5.06it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-612dcc767928>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict_on_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'30'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'below'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'brighten'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/users/riya/race/classifier_experiments/utils.py\u001b[0m in \u001b[0;36msystemic_brightening\u001b[0;34m(img, skeleton, thresh_type, intensity_change, brighten_sum, none_thresh, image_size)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;31m# depending on race\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmodified_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_skeletonize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskeleton\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# do I wanna threshold with skeletonization?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;31m# brightening code, brightening all pixels ABOVE 20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/riya/race/classifier_experiments/utils.py\u001b[0m in \u001b[0;36mprocess_skeletonize\u001b[0;34m(img, skeleton, image_size)\u001b[0m\n\u001b[1;32m     33\u001b[0m def process_skeletonize(img, skeleton, \n\u001b[1;32m     34\u001b[0m                         image_size): # (224, 224) typically\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_copy_dispatcher\u001b[0;34m(a, order, subok)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_copy_dispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predict_on_images(dataset_path, all_predictions, '30', False, 'below', 'brighten', 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13ef7ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
